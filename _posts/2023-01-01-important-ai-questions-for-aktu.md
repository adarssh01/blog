---
layout: post
title: "AI important questions."
subtitle: "List of AI important questions for aktu"
# date: 2020-01-26 23:45:13 -0400
background: '/img/posts/matrix.jpg'

---


### Q1:Describe the four categories under which AI is classified with examples.

Artificial intelligence (AI) is a broad field that includes a range of technologies and approaches for creating intelligent systems that can perform tasks without explicit human guidance. AI can be classified into four categories:

#### Reactive AI

Reactive AI is the most fundamental form of artificial intelligence. It is programmed to produce a predetermined output based on the input it receives. Reactive machines are incapable of learning actions or imagining the past or future because they always respond the same way to the same circumstances every time.

Reactive AI examples include:

Spam filters for our email that keep promotions and phishing attempts out of our inboxes  The Netflix recommendation engine Reactive AI was an enormous step forward in the history of the development of artificial intelligence, but these kinds of AIs cannot function beyond the tasks for which they were initially designed.  Deep Blue, the chess-playing IBM supercomputer that defeated world champion Garry Kasparov.  The Netflix recommendation engine As a result, they are naturally limited and deserving of improvement. From this foundation, scientists developed the subsequent type of AI.

#### Limited Memory AI

AI with limited memory AI with limited memory builds experiential knowledge by observing actions or data. It learns from the past. This sort of computer based intelligence utilizes authentic, observational information in blend with pre-modified data to make expectations and perform complex order errands. It is currently the AI type that is used the most.

For instance, autonomous vehicles use AI with limited memory to observe the speed and direction of other vehicles, enabling them to "read the road" and make any necessary adjustments. They are safer on the road thanks to this method of comprehending and interpreting incoming data.

However, AI with limited memory remains limited, as its name suggests. Autonomous vehicles work with short-lived information that is not stored in the vehicle's long-term memory.

#### Theory of Mind AI

Theory of Mind AI: Would you like to have a meaningful conversation with a human-like, emotionally intelligent robot? With AI theory, that is on the horizon.

Machines will acquire true decision-making capabilities comparable to those of humans with this kind of AI. Machines with hypothesis of psyche artificial intelligence will actually want to comprehend and recollect feelings, then change conduct in view of those feelings as they collaborate with individuals.

Because human communication is so fluid, the process of shifting behavior based on rapidly shifting emotions still poses a number of challenges to theory of mind AI. As we attempt to create increasingly emotionally intelligent machines, it is challenging to imitate.

Having said that, we are moving forward. Professor Cynthia Breazeal developed the Kismet robot head, which was able to recognize emotional signals on human faces and replicate those signals on its own face. Humanoid robot Sophia, created by Hanson Mechanical technology in Hong Kong, can perceive faces and answer associations with her own looks.

#### Self-aware AI

Self-aware AI Self-aware AI is the most advanced form of artificial intelligence. At the point when machines can know about their own feelings, as well as the feelings of others around them, they will have a degree of cognizance and insight like individuals. Additionally, this kind of AI will have needs, desires, and feelings.

The internal emotions and mental states of these machines will be self-aware. They will be able to draw conclusions like "I'm feeling angry because someone cut me off in traffic" that other types of AI cannot.

We haven't fostered this kind of complex computer based intelligence yet and don't have the equipment or calculations to help it

### Q2:List various components of natural language understanding process. Describe syntactic analysis and semantic analysis in brief.

Natural language understanding (NLU) is the process by which a computer system is able to understand and interpret human language. NLU involves a number of different components and techniques, including:

-   Text tokenization: This involves breaking a piece of text down into smaller units, such as individual words or punctuation marks.

-   Part-of-speech tagging: This involves identifying the grammatical role of each word in a sentence, such as whether it is a noun, verb, or adjective.

-   Named entity recognition: This involves identifying and labeling entities in a piece of text, such as people, organizations, or locations.

-   Stemming and lemmatization: These techniques involve reducing words to their base form, so that related words can be identified and grouped together.

-   Syntactic parsing: This involves analyzing the structure of a sentence and identifying the relationships between the words and phrases in it.

-   Semantic analysis: This involves understanding the meaning of the words and phrases in a sentence, and how they relate to each other.

Syntactic analysis is a subcomponent of NLU that involves analyzing the structure of a sentence and determining the relationships between the words and phrases in it. It involves identifying the syntactic structure of a sentence, such as the subject, verb, and object, and determining how these elements relate to each other. Syntactic analysis can be used to identify the grammatical role of each word in a sentence, such as whether it is a noun, verb, or adjective, and to identify the relationships between the words, such as whether they are modifying or modifying other words.

Semantic analysis is another subcomponent of NLU that involves understanding the meaning of the words and phrases in a sentence and how they relate to each other. It involves identifying the meanings of individual words and phrases and determining how they fit into the overall meaning of the sentence. Semantic analysis can be used to identify the relationships between words and phrases, such as whether they are synonyms or antonyms, and to identify the overall meaning of the sentence.

Both syntactic and semantic analysis are important components of NLU, as they allow computer systems to understand and interpret the meaning of human language. They are often used together, with syntactic analysis providing the structure and context for semantic analysis, and semantic analysis providing the meaning and interpretation of the words and phrases in a sentence.

### Q3:Explain resolution in predicate logic with suitable example.

Resolution is a rule of inference in predicate logic that allows us to derive new logical statements from a set of given statements. The resolution rule allows us to conclude that two statements are logically equivalent, or that one statement follows logically from the other, based on the structure and content of the statements.

To understand resolution in predicate logic, it is helpful to consider an example. Suppose we have the following two statements in predicate logic:

P: "All birds can fly."

Q: "Some birds are not able to fly."

We can use the resolution rule to derive a new statement from these two statements. Specifically, we can derive the statement:

R: "There exists at least one bird that cannot fly."

To derive this statement using the resolution rule, we first need to negate the first statement, P, to get the negation of P: "It is not the case that all birds can fly." This negation is equivalent to the statement: "There exists at least one bird that cannot fly."

Next, we can use the resolution rule to conclude that the negation of P, "There exists at least one bird that cannot fly," is logically equivalent to the second statement, Q, "Some birds are not able to fly." This conclusion follows from the fact that the negation of P and Q are logically equivalent statements, as they both assert that there exists at least one bird that cannot fly.

In this way, the resolution rule allows us to derive new logical statements from a given set of statements, by negating one of the statements and using it to resolve the other statement. This can be a useful tool for making logical conclusions and solving problems in predicate logic.

### Q4:Define decision tree? Explain itâ€™s with suitable example.

A decision tree is a graphical representation of a decision-making process that is used to determine the best course of action based on a set of conditions or variables. It is a tree-like model that consists of a series of nodes, with each node representing a decision or an outcome. The branches of the tree represent the possible consequences or outcomes of each decision.

Decision trees are often used in machine learning and data analysis to classify or predict outcomes based on a set of input data. They can be used to solve a wide range of problems, including classification, regression, and optimization problems.

Here is an example of a simple decision tree:

Suppose we are trying to decide whether to go to the beach or to a museum on a given day. We might consider the following factors:

* The weather forecast for the day (sunny, cloudy, or rainy)
* The time of day (morning, afternoon, or evening)
* The cost of admission to the beach or museum

Based on these factors, we could create a decision tree as follows:

* If the weather is sunny and it is the morning or afternoon, go to the beach.
* If the weather is cloudy and it is the morning or afternoon, go to the beach if the cost of admission is low, or go to the museum if the cost of admission is high.
* If the weather is rainy, go to the museum.

This decision tree allows us to make a decision based on the values of the three input variables: weather, time of day, and cost of admission. By following the branches of the tree, we can determine the best course of action based on the specific values of these variables.

Decision trees can be used to solve more complex problems as well, by adding additional nodes and branches to the tree to represent additional decision points and outcomes. They can also be used in combination with other machine learning algorithms to improve the accuracy of predictions and classifications.

### Q5:How can use Expectation-Maximization (EM Algorithm) in machine learning? Explain with appropriate example.

Expectation-Maximization (EM) is an iterative method used to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM algorithm is particularly useful when the statistical model has latent variables because it allows us to estimate the values of these latent variables even though we do not observe them directly.

Here is an example of how the EM algorithm can be used in machine learning:

Suppose we have a dataset of text documents, and we want to use this data to build a model that can classify each document as belonging to one of two categories: "sports" or "politics." We could build a model that uses the words in each document as features, and trains a classifier such as a logistic regression model to predict the category of each document.

However, this model might not work very well because there may be many words that are common to both sports and politics documents. For example, the word "game" could appear in both sports and politics documents, and the word "team" could also appear in both types of documents.

To improve the performance of our model, we could use the EM algorithm to identify latent topics in the documents, and use these topics as features instead of the individual words. The EM algorithm would iteratively estimate the values of the latent topics and the weights of the topics for each document, until it converges to a local maximum.

Once the EM algorithm has completed, we could use the learned latent topics as features to train a classifier, such as a logistic regression model, to predict the category of each document. This model would likely perform better than a model that uses the individual words as features, because the latent topics capture the underlying themes in the documents more effectively than the individual words.

### Q6:Explain DFS algorithm with suitable example.

The depth-first search (DFS) algorithm is a graph traversal algorithm that explores as far as possible along each branch before backtracking. It involves keeping a stack of nodes that need to be explored and popping the top node from the stack to visit it. If the popped node has unvisited neighbors, it pushes them onto the stack and repeats the process.

Here is a step-by-step description of the DFS algorithm:

1.  Initialize a stack to keep track of the nodes to visit, and a set to keep track of the visited nodes.
2.  Push the root node onto the stack and add it to the visited set.
3.  While the stack is not empty:
4.  Pop the top node from the stack.
5.  For each unvisited child of the node:
    1. Add the child to the visited set.
    2. Push the child onto the stack.
6.  Return the visited set.

Example:

``` python 
def dfs(root):
  stack = [root]
  visited = set()
  while stack:
    node = stack.pop()
    if node not in visited:
      visited.add(node)
      for child in node.children:
        stack.append(child)
  return visited

root = get_root_node()
visited = dfs(root)

```

### Q7:Define a well-formed formula (wff) and List some of the rules of inference.

A well-formed formula (wff) is a formula in a formal language that conforms to the rules of syntax for that language. In other words, it is a formula that is constructed according to the grammatical rules of the language, so that it is considered "well-formed" or "well-structured".

Some rules of inference are:

1.  Modus ponens: If A implies B, and A is true, then B is true.
2.  Modus tollens: If A implies B, and B is false, then A is false.
3.  Disjunctive syllogism: If A or B is true, and A is false, then B is true.
4.  Hypothetical syllogism: If A implies B, and B implies C, then A implies C.
5.  Addition: If A is true, then A or B is true.
6.  Simplification: If A and B are true, then A is true.
7.  Conjunction: If A and B are true, then A and B are true.
8.  Resolution: If A or B is true and A or C is true, then B or C is true.

### Q8:What are Statistical learning models? Show with suitable example.

Statistical learning models are mathematical models that are used to predict a response or outcome based on one or more predictor variables. These models are based on statistical principles and are fitted to data using a process called "learning", in which the model's parameters are adjusted to minimize the error between the predicted values and the observed values in the data.

There are many different types of statistical learning models, including linear models, nonlinear models, parametric models, nonparametric models, and ensemble models. These models can be used for a variety of purposes, such as classification, regression, density estimation, and clustering.

Some examples of statistical learning models include:

1.  Linear regression: A model that predicts a continuous response variable as a linear function of one or more predictor variables.
2.  Logistic regression: A model that predicts a binary response variable as a function of one or more predictor variables.
3.  Decision trees: A model that predicts a response variable based on a series of decision rules learned from the data.
4.  Support vector machines: A model that finds a hyperplane in a high-dimensional space that maximally separates the classes in a classification problem.
5.  K-means clustering: A model that divides a dataset into a specified number of clusters based on the similarity of the data points.

### Q9:Define PCA. Differentiate between Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA).

Principal Component Analysis (PCA) is a statistical technique for reducing the dimensionality of a dataset while preserving as much of the variation as possible. It does this by projecting the data onto a lower-dimensional space, called the principal components, which are the directions of maximum variance in the data.

-   PCA is an unsupervised technique, which means that it does not use any information about the class labels of the data. It finds the directions (called "principal components") in the data that have the highest variance, and projects the data onto these directions. The goal of PCA is to capture as much of the variance in the data as possible, with the goal of data visualization or reducing the dimensionality of the data before applying machine learning algorithms.

-   On the other hand, LDA is a supervised technique, which means that it uses the class labels of the data to find the directions that will maximize the separation between the different classes. LDA is often used for classification tasks, as it can provide a lower-dimensional representation of the data that is more discriminative for the class labels.

-   In summary, PCA is a technique for capturing the variance in a dataset, while LDA is a technique for maximizing the separation between different classes.

### Q10:Explain state space approach for solving any AI problem

The state space approach is a method for solving artificial intelligence (AI) problems that involves representing the problem as a set of states and transitions between those states. The goal is to find a sequence of transitions that leads from an initial state to a goal state, or to determine that no such sequence exists.

To use the state space approach, the problem must be represented as a graph, with the states of the problem represented as nodes and the transitions between states represented as edges. The initial state and the goal state are identified, and a search algorithm is used to explore the graph and find a path from the initial state to the goal state.

The state space approach is widely used in AI and can be applied to a wide range of problems, such as planning, scheduling, and decision making. It is particularly useful for problems that can be represented as graphs, where the states and transitions have a clear meaning and there is a clear goal to be achieved.

Here is an example of using the state space approach to solve a simple problem:

Suppose we have a robot that is trying to navigate through a maze. We can represent the maze as a state space, with the states being the different locations in the maze and the transitions being the possible movements of the robot. The initial state is the starting location of the robot, and the goal state is the location of the exit. Using a search algorithm such as breadth-first search or depth-first search, we can find a path from the initial state to the goal state that represents a solution to the problem.

In this example, the state space approach allows us to represent the problem of navigating through the maze in a clear and structured way, and to use a search algorithm to find a solution to the problem.

[Question papers](https://drive.google.com/drive/folders/1v3J-k2Dnz362ohj9_O1AlCqSEhycJUbl?usp=sharing){: .btn}